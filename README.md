# Fashion-MNIST VAE and CVAE Project

This notebook explores Variational Autoencoders (VAEs) and Conditional Variational Autoencoders (CVAEs) for image generation and reconstruction on the Fashion-MNIST dataset. It provides a comprehensive workflow from data preparation and exploratory data analysis (EDA) to model training, visualization of generated samples, latent space exploration, and quantitative evaluation using metrics like Fréchet Inception Distance (FID) and a classifier-based controllability metric.

## Overview

The project aims to demonstrate the capabilities of VAEs and CVAEs in unsupervised and conditional image generation. It highlights how different architectural choices and hyperparameters (like latent dimension and β-VAE regularization) impact model performance and the quality of generated images. The notebook also introduces advanced evaluation techniques beyond visual inspection, such as FID for generation quality and a custom metric for CVAE's class controllability.

## What’s Inside

The notebook is structured into four main phases:

### Phase 1: Data Preparation and EDA

This phase handles the loading, splitting, and initial exploration of the Fashion-MNIST dataset.

-   **`FashionMNISTManager`**: A custom class designed to manage all data operations. It handles downloading the Fashion-MNIST dataset, splitting it into training, validation, and test sets, and creating PyTorch `DataLoader` objects for efficient batch processing.
-   **Sanity Check**: Visualizes a random batch of images from the training set to confirm correct data loading and preprocessing.
-   **Exploratory Data Analysis (EDA)**: Includes visualizations of class distribution across the dataset and pixel intensity histograms to understand the statistical properties of the image data.

### Phase 2: Variational Autoencoder (VAE) Training and Evaluation

This phase implements and trains various VAE models, exploring different configurations.

-   **`BaseVAE`**: Defines a basic VAE architecture using fully connected layers for both the encoder and decoder.
-   **`ConvVAE`**: An improved VAE architecture employing convolutional layers, which are generally more effective for image data.
-   **`VAETrainer`**: A class encapsulating the training loop for VAEs, including the calculation of the variational lower bound loss (reconstruction loss + KL divergence).
-   **`VAEVisualizer`**: Provides utilities to display reconstructed images and generate new samples by sampling from the latent space.
-   **FID Score Calculation**: Implements the Fréchet Inception Distance using a provided pre-trained Fashion-MNIST classifier (`fashion_resnet18_classifier.pt`) to quantitatively assess the quality and diversity of generated images.

**Key Results from Phase 2 (FID Scores):**

-   **Base VAE (latent_dim=2, beta=1)**: FID Score: **23.8351**
-   **Improvement 1: Beta=0.5 VAE (latent_dim=2, beta=0.5)**: FID Score: **28.3813** (Slightly worse, indicating lower beta might not always be better without other changes)
-   **Improvement 2: Latent Dimension 10 VAE (latent_dim=10, beta=1)**: FID Score: **8.6994** (Significant improvement by increasing latent capacity)
-   **Improvement 3: Convolutional VAE (latent_dim=10, beta=1)**: FID Score: **8.7528** (Achieved high quality, comparable to the higher latent dimension fully-connected VAE)

### Phase 3: Latent Space Traversal

This phase visually explores the latent space of the trained VAE models.

-   **`latent_traversal`**: Displays images generated by systematically varying a single dimension of the latent space while keeping others constant, revealing how individual latent dimensions influence image features.
-   **`latent_traversal_pairwise`**: Visualizes how images change when two latent dimensions are varied simultaneously, providing insights into the correlation and interaction between different latent factors.
-   Experiments are conducted with different `beta` values (0.5, 1.0, 4.0) with `latent_dim=5` to observe their impact on latent space smoothness and image quality.

### Phase 4: Conditional Variational Autoencoder (CVAE)

This phase implements and evaluates a CVAE, which allows for class-conditioned image generation.

-   **`ConcatConditionalVAE`**: A convolutional CVAE architecture where class labels are embedded and concatenated to both the encoder's hidden states and the decoder's input, enabling control over the generated image's class.
-   **`CVAETrainer`**: A specialized trainer for the CVAE, handling the conditional aspect of the model.
-   **`CVAEVisualizer`**: Provides methods to visualize CVAE reconstructions, generate class-specific samples, and compare them with unconditional VAE outputs.
-   **Controllability Evaluation**: Assesses how accurately a pre-trained classifier can identify the class of images generated by the CVAE for a target class. This provides a quantitative measure of the CVAE's ability to generate class-specific images.

**CVAE Evaluation (latent_dim=32, beta=1):**

-   **Test Set Metrics**:
    -   Total Loss: 236.3732
    -   Reconstruction Loss: 222.2314
    -   Regularization (KLD): 14.1418
-   **Controllability Evaluation**:
    -   Overall Average Accuracy: **71.90%** (Indicates the CVAE can largely generate images aligning with intended classes, though some classes like 'Coat' and 'Sandal' show lower accuracy, suggesting challenges in generating distinctive features for these categories.)

## Requirements

To run this notebook, you will need the following Python libraries:

-   `numpy`
-   `random`
-   `matplotlib.pyplot`
-   `seaborn`
-   `torch`
-   `torchvision`
-   `scipy`
-   `Pillow` (often a dependency of torchvision, for image processing)

You will also need the pre-trained classifier model file: `fashion_resnet18_classifier.pt`.

## Setup (Colab & Local)

### Google Colab

1.  **Open the Notebook**: Upload the `.ipynb` file to Google Colab or open it directly if it's hosted online.
2.  **Enable GPU (Optional but Recommended)**: Go to `Runtime -> Change runtime type` and select `GPU` as the hardware accelerator.
3.  **Install Dependencies**: Run the following in a code cell (or if `pip install` commands are present at the beginning of the notebook):
    ```bash
    !pip install torch torchvision numpy matplotlib seaborn scipy
    ```
4.  **Download Classifier**: Ensure the `fashion_resnet18_classifier.pt` file is available in your Colab environment. You might need to upload it manually or download it
5.  **Run Cells**: Execute the notebook cells sequentially.

### Local Environment

1.  **Clone Repository (if applicable)**:
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```
2.  **Create Virtual Environment (Recommended)**:
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows: .\venv\Scripts\activate
    ```
3.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt # If a requirements.txt is provided, otherwise:
    pip install torch torchvision numpy matplotlib seaborn scipy
    ```
    (Ensure you install the correct PyTorch version for your CUDA setup if using a GPU).
4.  **Download Classifier**: Place the `fashion_resnet18_classifier.pt` file in the same directory as your notebook or ensure its path is correctly specified.
5.  **Run Jupyter**:
    ```bash
    jupyter notebook
    ```
    Then open the notebook file (`.ipynb`) in your browser and execute cells sequentially.

## How to Run

1.  **Ensure all requirements are met** and the `fashion_resnet18_classifier.pt` file is accessible.
2.  **Execute the notebook cells sequentially** from top to bottom.
3.  The notebook will perform the following actions:
    -   Download and prepare the Fashion-MNIST dataset.
    -   Perform EDA and sanity checks.
    -   Train the `BaseVAE` model (latent_dim=2, beta=1).
    -   Visualize reconstructions and generated samples from the `BaseVAE`.
    -   Report test metrics and FID score for the `BaseVAE`.
    -   Train and evaluate improved VAE models (beta=0.5, latent_dim=10, ConvVAE).
    -   Perform latent space traversals for different `beta` values.
    -   Train the `ConcatConditionalVAE`.
    -   Visualize CVAE reconstructions and class-specific generations.
    -   Evaluate CVAE controllability using the provided classifier.

## Data

The project utilizes the **Fashion-MNIST** dataset, a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. It serves as a direct drop-in replacement for the original MNIST dataset. The `FashionMNISTManager` class automatically handles downloading this dataset.

## Configuration

Key configurable parameters within the notebook include:

-   **`FashionMNISTManager`**:
    -   `data_root`: Directory to store downloaded data (default: `./data`).
    -   `batch_size`: Number of samples per batch during training (default: `128`).
    -   `seed`: Random seed for reproducibility (default: `42`).
    -   `val_ratio`: Proportion of the training data to use for the validation set (default: `0.1`).
-   **VAE Models (`BaseVAE`, `ConvVAE`, `ConcatConditionalVAE`)**:
    -   `latent_dim`: Dimensionality of the latent space (e.g., `2`, `10`, `32`).
    -   `num_classes`: Number of classes for CVAE (fixed at `10` for Fashion-MNIST).
-   **Trainer Classes (`VAETrainer`, `CVAETrainer`)**:
    -   `learning_rate`: Optimizer learning rate (default: `1e-3`).
    -   `beta`: Weight for the KL divergence term in the loss function (β-VAE parameter, e.g., `0.5`, `1.0`, `4.0`).
    -   `epochs`: Number of training epochs (e.g., `20`, `30`).

These parameters can be adjusted directly in the code cells where models and trainers are instantiated.

## Outputs

The notebook generates various outputs, including:

-   **Console Logs**: Detailed training progress (loss, epoch numbers) and informational messages.
-   **Plots**:
    -   Class distribution histograms.
    -   Pixel intensity histograms.
    -   Sanity check images (original batch samples).
    -   VAE reconstruction comparisons (original vs. reconstructed images).
    -   VAE generated samples from random latent vectors.
    -   Latent space traversal plots (showing image variations along single or pairwise latent dimensions).
    -   CVAE reconstruction comparisons.
    -   CVAE generated samples per class (grid of class-conditioned images).
    -   Visual comparison of CVAE vs. unconditional VAE generated samples.
    -   Bar plot visualizing CVAE controllability (classifier accuracy per class).
-   **Metrics**:
    -   Training and validation loss history.
    -   Test set metrics (Total Loss, Reconstruction Loss, KLD).
    -   Fréchet Inception Distance (FID) scores for different VAE models.
    -   CVAE controllability report (per-class and overall classifier accuracy).

## Results

The experiments demonstrate several key findings:

-   **Impact of Latent Dimension**: Increasing the latent dimension from 2 to 10 significantly improved the FID score for VAEs (from ~23.8 to ~8.7), indicating better generation quality and diversity.
-   **Convolutional Architectures**: Moving from fully connected to convolutional VAE (`ConvVAE`) maintained high generation quality, with a FID score comparable to the best fully-connected VAE with a higher latent dimension (8.75 vs 8.70).
-   **Beta-VAE Tuning**: A `beta` of 0.5 for the `BaseVAE` (latent_dim=2) resulted in a worse FID (28.38) compared to `beta=1` (23.83), highlighting the sensitivity of VAE performance to the regularization strength. Latent space traversals further illustrate how `beta` affects the smoothness and interpretability of the latent space.
-   **CVAE Controllability**: The `ConcatConditionalVAE` successfully learned to generate class-specific images, achieving an overall classifier accuracy of **71.90%** on generated samples. While classes like 'Trouser' and 'Bag' showed very high accuracy (e.g., 99%), certain classes such as 'Coat' and 'Sandal' proved more challenging to generate distinctively, resulting in lower accuracies. This suggests areas for further model improvement.

## Troubleshooting

-   **CUDA out of memory**: If you encounter this error, try reducing the `batch_size` parameter in the `FashionMNISTManager` or reduce the number of images generated for FID calculation or visualization.
-   **ModuleNotFoundError**: Ensure all required Python libraries are installed.
-   **Classifier not found**: Make sure `fashion_resnet18_classifier.pt` is in the correct directory or its path is specified accurately.
-   **Slow training**: If not using Colab GPU, training on CPU can be very slow. Consider enabling GPU runtime or using a machine with a dedicated GPU.

